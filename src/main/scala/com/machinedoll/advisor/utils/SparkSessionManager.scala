package com.machinedoll.advisor.utils

import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

class SparkSessionManager {
  val confContext = new ConfigLoader()

  def createSparkStreamContext(): StreamingContext = {
    val sparkConf = new SparkConf()
      .setMaster("local[2]")
      .setAppName(confContext.load.getString("advisor.app.name"))

    val ssc = new StreamingContext(sparkConf,
      Seconds(confContext.load.getInt("advisor.app.spark.streaming.seconds")))
    return ssc
  }

  def createSparkContext(master: String): SparkContext = {
    val sparkConf = new SparkConf()
//      .setMaster(master)
      .setAppName(confContext.load.getString("advisor.app.name"))
//      .set("spark.driver.host", "127.0.0.1")

    val sc = new SparkContext(sparkConf)
    return sc
  }

  def createSparkSession(): SparkSession = {
    val spark = SparkSession.builder
      .appName("My Spark Application")  // optional and will be autogenerated if not specified
      .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
      .config("spark.sql.warehouse.dir", "target/spark-warehouse")
      .getOrCreate
    return spark
  }

}
